# Analysis

## Preprocessing:

* We tried to tokenize the data as word as well as the characters (in seperate implementa-
tions). The character level tolenization is quite elementary where each character of the
poem as an input one-hot vector (including the spaces and punctuations). The end of
line is represented by ’ ’. We used this symbol because this was nowhaere to be found
in the dictionary. For the character-based LSTM, we used average character length of
the poems as length of sequence naively, this will work as every character has key in the
vocablury (even line break). We are using Stochastic Gradient Descent without batch
implementation (since, sie of dataset is small SGD will genralize better compared to
batch gradient descent).

* Finally we decided to go with word tokenized input for the LSTM. Here we initially
tried by incorporating the space as a seperate word. But in that case the model was not
able learn i.e., it was generalizing every word as space since, that is present throughout
the document. You can see the histogram for the unique words generated. Different
sequences for this tokenization were tried. We tried keeping each line as a single sequence
(with length of input as length of longest word-wise). We also tried couplet. The couplet
seemed to catch the general structure of the poem. This was because we specified the line
break seperately for even as well as odd lines. We decided to keep the hyphenated as it
is after a short test-run. Since, hyphenated words change the meaning of the poem, the
model was inserting it in unwanted spaces. This didn’t happen when using the complete
hyphenated word as a token. We seperated the punctuations at the end of words to make
it seperate token.

* The syllable dictionary could have helped us better genarlize the model by concatenating
this information with sequence. Same could be said for NLTK library’s POS tags. These,
specify the type of word. But since, current task was to generate, it might not have been
helpful. We weren’t able to test this implementation due to installation constraints. We
did not use bigrams as a token because it would only be effective only with adjectives
and increase the number of tokens quite drastically.

## Baseline:

* We implemented the character-based implemented with single layer and tested the model
with different number of units. We tried with 128, 200 units. The model with 128
seemed to perform better. We also tried both batch gradient descent as well as SGD. But
as the size of the data is small SGD seemed to generalise better and was able to produce
some meaningful sentence.

* We used consecutive sequences of length as average length of the lines (42 character)
(since, we have line breaks this was plausible). Initially we used 40 character. There
was no noticable difference between these implementation. Since, the model was quite
elementary we could only tune number of epochs (increased till loss converged), number
of units in LSTM, some basic preprocessing (could have use different line breaks for
different part of poem).

* As instructed we draw the softmax samples from the trained model with temperature
parameter. This seemed the measure of the randomness.

* The poems produced by the final did not contain much structure along the lines of a
sonnet and rhyme scheme. But it seemed to generate some meaningful sentences. On
running the notebook one get all poems generated by the model when we provide it with
first line of the poem.

* For the given the first line ”shall i compare thee to a summer’s day?” the poems gen-
erated by the model is present in the notebook for all temperature. We know that on
decreasing the temperature the randomness in the sampling increases. This was evi-
dent as really low temperature (0.25) produced different outputs than the ones learned
(which did not make much). Randomness is not good for a baseline model as its capac-
ity is quite, so, increasing the randomness might lead to production of gibberish. On
increasing the temperature the model was able to produce some meaningful sentences.
But the model was not able to learn the structure of the poem. This was evident as the
model was not able to produce the rhyming words.

* So, to conclude this model only could produce somewhat meaningful sentences and
could not learn sentence and/or sonnet structure.

## Improvement:

* On using the word-embeddings the performance of the model improved. This was not
possible only by changing the tokenization. We also needed to make model much denser
so that it was able to learn sensible sonnet structure.

* The model with word-embedding would not be able to learn new words, this was only
possible for character-based LSTM. But since, the dataset was quite large it was able to
learn most of the words.

* Compared to baseline the model with word-embedding was able to learn the structure
of the poem atleast quite well on the surface. It was able to produce rhyme scheme
(partially, for a stanza) in some cases. The perplexity metric for character based LSTM
was higher but it did not help here (since, learning structure more important).

* We could significantly increase the performance of the model on incorporating sylla-
ble information along with NLTK for feeding through input and using the attention-
mechanism as there are synonyms present in the poem dataset which had different syl-
lable count. This was not possible with the baseline model. (Could not implement due
to installation and time constraints).
